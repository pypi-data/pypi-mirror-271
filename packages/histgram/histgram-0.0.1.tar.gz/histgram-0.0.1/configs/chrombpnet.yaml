model:
  n_filters: 256,
  n_layers: 8,
  profile_output_bias: True,
  count_output_bias: True,
  name: None,
  'in_window': 2114,
  'out_window': 1000,
  'max_jitter': 128,
  'reverse_complement': True,

  'validation_iter': 100,
  'lr': ,
  'alpha': 10,
  'beta': 0.5,
  'verbose': False,
  'bias_model': None,

  'min_counts': None,
  'max_counts': None,

data:
  max_epochs: 50,
	batch_size: 64,
	training_chroms: ['chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 
		'chr9', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 
		'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX'],
	validation_chroms: ['chr8', 'chr10'],

	'sequences': None,
	'loci': None,
	'negatives': None,
	'signals': None,
	'random_state': None,

	# Fit bias model
	'bias_fit_parameters': {
		'n_filters': None,
		'n_layers': 4,
		'alpha': None,
		'max_counts': None,
		'loci': None,
		'verbose': None,
		'random_state': None
	}

trainer:
    # seed: 42
    max_epochs: 100
    accelerator: "gpu"
    devices: 1
    logger:
        class_path: lightning.pytorch.loggers.TensorBoardLogger
        init_args:
            save_dir: "lightning_logs"
            name: "BPNet"
            # version: "0.1"
            # sub_dir: "K562_h3k27ac"
    callbacks:
    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #       monitor: "val_loss"
    #       mode: "min"
    #       save_top_k: 1
    #       dirpath: "lightning_logs"
    #       filename: "checkpoint"
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
          monitor: "val_loss"
          patience: 10
          mode: "min"
    # - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    # - class_path: pytorch_lightning.callbacks.GPUStatsMonitor
    # - class_path: pytorch_lightning.callbacks.ProgressBar


optimizer:
    class_path: torch.optim.Adam
    init_args:
        lr: 0.001
        # weight_decay: 0.01

# lr_scheduler:
#     class_path: torch.optim.lr_scheduler.CosineAnnealingLR
#     init_args:
#         T_max: 10