---
# trainer.yaml - training recipe configuration for a single experiment

# run_name: str, no default - the name to give the run in Weights & Biases
#     corresponding to this experiment; if this is not provided, a random 
#     8-character name will be generated instead
# run_name: "test_run"

# description: string (optional, default None) - a written description of the purpose of this HPO run
#     logged to the sweep summary in Weights & Biases
description: "A test run with the demo configuration files."

# optimizer settings
optimizer:
  # name: string - the optimizer class to load and use
  #    for Sparsey models you should always use "hebbian"
  name: hebbian
  params: {}

# metrics: list of metrics to compute during an experiment
#     system Metric classes are defined in core/metrics
#     if you add a Metric class to the system, you also need to add a corresponding
#     metric schema in cli/config_validation/saved_schemas/metric
#     in order for it to be detected here and pass validation
metrics:
  - name: basis_set_size
    save: True
    best_value: max_by_layerwise_mean
  - name: basis_average
    save: True
  - name: num_activations
    save: True
    reduction: layerwise_mean
  - name: feature_coverage
    save: True
  - name: match_accuracy
    save: True
  - name: num_activations
    save: True

# training-related hyperparameters
training:
  # num_epochs: int > 0
  #     number of epochs to train for
  #     for Sparsey single-shot learning this will always be 1
  #     but the setting exists for potential future compatibility
  num_epochs: 1

  # dataloader settings for loading items from the dataset
  dataloader:
    # batch_size: int > 0
    #    number of input items per batch
    batch_size: 1
    # shuffle: bool
    #    whether to randomize the inputs as they are drawn from the dataloader
    shuffle: True

eval:
  # dataloader settings for loading items from the dataset
  dataloader:
    # batch_size: int > 0
    #    number of input items per batch
    batch_size: 16
    # shuffle: bool
    #    whether to randomize the inputs as they are drawn from the dataloader
    shuffle: True

# use_gpu: bool, default True if the system has a compatible GPU, otherwise False
#     whether to use a GPU to accelerate system performance, if one is available
#     (the system supports CUDA and MPS backends)
#     in general, you are better off leaving this out and letting the system
#     automatically use a GPU if it is available) unless you specifically want
#     to refuse to start the run in the absence of a GPU (set True) or
#     force the system to never use a GPU even if it is present (set False)
# use_gpu: True
...