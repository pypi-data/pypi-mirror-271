# -*- coding: utf-8 -*-
"""
DataStorer: Saves data to Weights & Biases and the system database (Firestore)
"""
import json

import numpy as np
import torch
import wandb

from sparseypy.core.results.training_result import TrainingResult
from sparseypy.core.results.training_result import TrainingStepResult
from sparseypy.core.results.hpo_result import HPOResult
from sparseypy.core.results.hpo_step_result import HPOStepResult
from sparseypy.access_objects.models.model import Model
from sparseypy.core.db_adapters import DbAdapterFactory
from sparseypy.core.metrics import reductions

class DataStorer:
    """
    DataStorer: Stores data to Weights & Biases
    """
    # static variables
    is_initialized = False
    wandb_config = {}
    firestore_config = {}


    def __init__(self, metric_config: dict):
        # ensure the config has been initialized
        if not DataStorer.is_initialized:
            raise ValueError("You must call DataStorer.configure() before intializing DataStorer objects.")

        # configure saved metrics?
        self.saved_metrics = [metric["name"] for metric in metric_config if metric["save"] is True]
        # create API client
        self.api = wandb.Api()

        self.wandb_resolution = DataStorer.wandb_config["data_resolution"]

        self.step_cache = {
            "training": [],
            "evaluation": []
        }

        self.db_adapters = [
                DbAdapterFactory.create_db_adapter(
                        db_adapter_name=write_database["name"],
                        config=write_database,
                        metric_config=metric_config
                    )
                for write_database in DataStorer.db_config["write_databases"]
            ]

    @staticmethod
    def configure(ds_config: dict):
        """
        Configures the DataStorer by logging into Weights & Biases and
        initializing its database connection.

        Because all configuration is tracked inside firebase_admin and 
        wandb, calling this method also configures the DataFetcher.

        Args:
            ds_config (dict): the validated system.yaml configuration
        """
        if not DataStorer.is_initialized:

            # initialize W&B
            wandb.login(key=ds_config['wandb']['api_key'], verify=True)

            DataStorer.wandb_config = ds_config['wandb']
            DataStorer.db_config = ds_config["database"]

            DataStorer.is_initialized = True


    def save_model(self, experiment: str, m: Model, model_config: dict):
        """
        Saves a model to Weights & Biases.

        Args:
            experiment (str): the experiment ID to which the model should be saved
            m (Model): the model object to be saved
        """
        if DataStorer.wandb_config["save_models"]:
            # WEIGHTS & BIASES
            # this section creates the artifact manually because doing so provides
            # more flexibility than using link_model()
            # instance name: the unique name of this specific model version
            instance_name = experiment + "-model"
            # model name: the user-defined model name that this model is a version of
            model_name = model_config["model_name"] if model_config["model_name"] else instance_name
            # model description: string description to log into W&B model registry
            model_description = model_config["model_description"]
            if model_description is None:
                model_description = f"Automatically generated by run {experiment}."

            # create the model artifact
            model_artifact = wandb.Artifact(
                name=instance_name,
                type="model",
                description=model_description,
                metadata={
                    'source_run': experiment,
                    'source_path': wandb.run.path
                }
            )

            # add the state dict
            with model_artifact.new_file("model.pt", mode="wb") as f:
                torch.save(m.state_dict(), f)

            # add the model config file (required for rehydration)
            with model_artifact.new_file("network.yaml", encoding="utf-8") as f:
                json.dump(model_config, f)

            # log to W&B and link to the correct model registry entry
            wandb.log_artifact(model_artifact).wait()
            wandb.run.link_artifact(model_artifact, f"model-registry/{model_name}")

            # DATABASE
            for db_adapter in self.db_adapters:
                db_adapter.save_model(
                    experiment=experiment,
                    m=m,
                    model_config=model_config,
                    wandb_location=model_artifact.source_qualified_name
                )


    def _save_wandb_training_step(self, parent: str, result: TrainingStepResult):
        """
        Saves a single training or evaluation step to Weights & Biases.

        Args:
            parent (str): the experiment ID to which to log this step
            result (TrainingStepResult): the step results to save
        """
        base_step = wandb.run.step
        # for each item in the TrainingStepResult's batch
        for batch_index in range(result.batch_size):
            # create the summary dict
            summary_dict = {}

            # gather the saved metrics for summary in W&B and full storage in the DB
            for metric_name, metric_val in result.get_metrics().items():
                if metric_name in self.saved_metrics:
                    summary_dict[metric_name] = reductions.average_nested_data(
                        torch.select(metric_val, dim=1, index=batch_index)
                    )

            # log higher-resolution data to W&B, if requested
            if self.wandb_resolution > 0:
                full_dict = {}
                # get the data for each metric
                for met_name, met_val in result.get_metrics().items():
                    # if that metric is requested for saving and is a Tensor
                    if met_name in self.saved_metrics and isinstance(met_val, torch.Tensor):
                        # then break out each layer as a separate metric for W&B
                        # using prefix grouping
                        for idx, layer_data in enumerate(
                            torch.select(met_val, dim=1, index=batch_index)
                        ):
                            layer_name = f"{met_name}/layer_{idx+1}"
                            full_dict[layer_name] = reductions.average_nested_data(layer_data)
                            # if the resolution is 2 (MAC-level)
                            # also log the MAC-level data with prefix grouping
                            if self.wandb_resolution == 2 and isinstance(layer_data, torch.Tensor):
                                for idy, mac_data in enumerate(layer_data):
                                    mac_name = f"{met_name}/layer_{idx+1}/mac_{idy+1}"
                                    if mac_data.dim() > 0:
                                        full_dict[mac_name] = reductions.average_nested_data(mac_data)
                                    else:
                                        full_dict[mac_name] = mac_data.cpu().item()
                # then log without updating the step (done when the summary is logged below)
                wandb.log(full_dict, commit=False)

            # save the summary to W&B
            wandb.log(
                summary_dict, # log the summary
                step=(base_step + batch_index), # to the corresponding step
                commit=(batch_index == result.batch_size - 1) # commit only on last step
            )


    def save_training_step(self, parent: str, result: TrainingStepResult):
        """
        Saves a single training step to Weights & Biases and Firestore.

        Args:
            parent (str): the experiment ID to which to log this step
            result (TrainingStepResult): the step results to save
        """
        self._save_wandb_training_step(parent, result)

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_training_step(parent, result)


    def save_evaluation_step(self, parent: str, result: TrainingStepResult,
                             log_to_wandb: bool = False):
        """
        Saves a single evaluation step to Weights & Biases and Firestore.

        Args:
            parent (str): the experiment ID to which to log this step
            result (TrainingStepResult): the step results to save
        """
        # WEIGHTS & BIASES
        # step-level evaluation data is not saved to Weigthts & Biases
        # due to platform limitations
        if log_to_wandb:
            self._save_wandb_training_step(parent, result)

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_evaluation_step(parent, result)


    def create_experiment(self, experiment: TrainingResult):
        """
        Creates a new entry for the current experiment in Firestore.
        
        Args:
            experiment (TrainingResult): the TrainingResult for the new experiment
            for which to create a database entry
        """

        # WEIGHTS & BIASES
        training_ds_config = experiment.get_config('training_dataset_config')
        eval_ds_config = experiment.get_config('evaluation_dataset_config')

        if training_ds_config is not None:
            description = training_ds_config.get('description', None)

            if description:
                wandb.run.summary[
                        "training_dataset_description"
                ] = description

        if eval_ds_config is not None:
            description = eval_ds_config.get('description', None)

            if description:
                wandb.run.summary[
                    "evaluation_dataset_description"
                ] = description

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.create_experiment(experiment=experiment)


    def save_training_result(self, result: TrainingResult):
        """
        Saves the summary-level training results for the current run
        to Firestore. 

        Only saves the training summary--you still need to save the individual 
        training steps by calling save_training_step().

        Args:
            result (TrainingResult): the completed training results
            to save
        """
        # WEIGHTS & BIASES
        # all data is already tracked in W&B so we don't need to save anything special here

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_training_result(result)


    def save_evaluation_result(self, result: TrainingResult):
        """
        Saves the summary-level evaluation results for the current run
        to Firestore. 

        Only saves the evaluation summary--you still need to save the individual 
        evaluation steps by calling save_evaluation_step().

        Args:
            result (TrainingResult): the completed evaluation results
            to save
        """
        # WEIGHTS & BIASES

        # gather the saved metrics for summary in W&B
        eval_dict = {
            # create a key for each saved metric containing the nested average
            # of the results of the metric for each step in the evaluation
            saved_metric: torch.mean(
                torch.tensor(
                    [
                        reductions.average_nested_data(
                            step.get_metric(saved_metric)
                        ) for step in result.get_steps()
                    ]
                )
            ) for saved_metric in self.saved_metrics
        }
        # then add those as "evaluation_" results to the W&B summary level
        for metric_name, metric_val in eval_dict.items():
            wandb.run.summary["evaluation_" + metric_name] = metric_val

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_evaluation_result(result)


    def save_hpo_step(self, parent: str, result: HPOStepResult):
        """
        Saves a single HPO step to Weights & Biases and Firestore.

        Saves objective data and HPO configuration to the run in
        both Weights & Biases and Firestore.

        Also marks this experiment in Firestore as belonging to the
        parent sweep and updates its best runs.
        
        Args:
            parent (str): the ID of the parent sweep in the HPO table
            that should be updated with this run's results
            result (HPOStepResult): the results of the HPO step to save
        """
        # WEIGHTS & BIASES
        # save the objective and HPO config to this experiment
        objective = result.get_objective()
        # FIXME correct duplicate logging, only log hpo_objective
        wandb.log(
                    {
                        'hpo_objective': objective["total"],
                        'objective_details': objective 
                    }
                )

        run = self.api.run(wandb.run.path)
        run.summary["hpo_configs"] = result.configs

        run.summary.update()

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_hpo_step(parent, result)


    def create_hpo_sweep(self, sweep: HPOResult):
        """
        Creates an entry in Firestore for the given HPO sweep.

        Stores basic metadata that Weights & Biases tracks automatically
        but needs to be manually created in Firestore for other
        storage functions (such as save_hpo_step()) to work correctly.

        Args:
            sweep (HPOResult): the sweep for which to create an entry
        """
        # WEIGHTS & BIASES automatically tracks this data

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.create_hpo_sweep(sweep)


    def save_hpo_result(self, result: HPOResult):
        """
        Saves the final results of an HPO run to Firestore and
        marks it as completed.

        Includes end times, best run ID, and an ordered list of runs
        by objective value.

        Does not save the individual steps--you need to use
        save_hpo_step() for that.

        Args:
            result (HPOResult): the results of the completed HPO sweep to
            summarize and save
        """

        # WEIGHTS & BIASES automatically tracks this already

        # DATABASE
        for db_adapter in self.db_adapters:
            db_adapter.save_hpo_result(result)


    def average_nested_data(self, data: torch.Tensor):
        """
        Averages an arbitrarily deep data structure
        and returns the result as a single value.

        Used here to reduce the granularity of data in order
        to store a single value for each step in W&B.

        Args:
            data (torch.Tensor): the (possibly nested) tensor
                containing the raw metric values computed.

        Returns:
            (float): a single value representing the averaged data
        """
        return reductions.average_nested_data(data)
